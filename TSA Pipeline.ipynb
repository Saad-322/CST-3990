{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c3a2c4",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fd930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import string,re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,f1_score,precision_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.ensemble import BaggingClassifier,AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import joblib, pickle\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(file_path):\n",
    "    if file_path == 'TSA_datasets/sentiment140.csv':\n",
    "        df = pd.read_csv('TSA_datasets/sentiment140.csv',encoding='latin-1',names=['sentiment','user_id','date','query','user','text'])\n",
    "        df = df[['text','sentiment']]\n",
    "        df['sentiment']=df['sentiment'].apply(lambda x: set_polarity(x))\n",
    "    elif file_path == 'TSA_datasets/IMDB Dataset.csv':\n",
    "        df = pd.read_csv(file_path,index_col=0)\n",
    "    else:\n",
    "        df = pd.read_csv(file_path,index_col=0)\n",
    "        df = pd.DataFrame(columns=['reviewText','overall'],data=df)\n",
    "        df['text'] = df['reviewText']\n",
    "        df['sentiment'] = df['overall']\n",
    "        df.drop(columns=['reviewText','overall'],axis=1,inplace=True)\n",
    "        df['sentiment']=df['sentiment'].apply(lambda x: set_polarity(x))\n",
    "        df.dropna(inplace=True)\n",
    "    return df.head(100)\n",
    "def set_polarity(sentiment):\n",
    "    if sentiment > 3:\n",
    "        return 'positive'\n",
    "    elif sentiment < 3:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b918a",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455440dc",
   "metadata": {},
   "source": [
    "### lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(df):\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e2125",
   "metadata": {},
   "source": [
    "### remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c83f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(df):\n",
    "    print('removing punctuation')\n",
    "    df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6a44f",
   "metadata": {},
   "source": [
    "### remove hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b401d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlinks_usernames(df):\n",
    "    print('removing hyperlinks and usernames')\n",
    "    df['text'] = df['text'].apply(lambda x: remove_h_u(x))\n",
    "    return df\n",
    "def remove_h_u(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links = re.findall(link_regex,text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0],'')\n",
    "    prefix = ['@']\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in prefix:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda433c",
   "metadata": {},
   "source": [
    "### remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(df):\n",
    "    print('removing emojis')\n",
    "    df['text'] = df['text'].apply(lambda x: remove_e(x))\n",
    "    return df\n",
    "def remove_e(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d3641",
   "metadata": {},
   "source": [
    "### remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf02533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df):\n",
    "    print('removing stop words')\n",
    "    df['text'] = df['text'].apply(lambda x: remove_sw(x))\n",
    "    return df\n",
    "def remove_sw(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        if word not in stop_words:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b517fa3d",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff6b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(df):\n",
    "    print('tokenizing')\n",
    "    df['text'] = df['text'].apply(lambda x: perform_t(x))\n",
    "    return df\n",
    "def perform_t(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a938ded",
   "metadata": {},
   "source": [
    "### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5aae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(df):\n",
    "    print('lemmatizing text')\n",
    "    df['text'] = df['text'].apply(lambda x: perform_l(x))\n",
    "    return df\n",
    "def perform_l(text):\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    text = [wnl.lemmatize(token) for token in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94662725",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335eb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(df):\n",
    "    print('stemming text')\n",
    "    df['text'] = df['text'].apply(lambda x: perform_s(x))\n",
    "    return df\n",
    "def perform_s(text):\n",
    "    sp = nltk.PorterStemmer()\n",
    "    text =[sp.stem(token) for token in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3122662a",
   "metadata": {},
   "source": [
    "### data cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2872a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(df):\n",
    "    lower_case(df)\n",
    "    remove_hyperlinks_usernames(df)\n",
    "    remove_punctuations(df)\n",
    "    remove_emojis(df)\n",
    "    remove_stopwords(df)\n",
    "    tokenization(df)\n",
    "    lemmatization(df)\n",
    "    stemming(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b248aa",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df):\n",
    "    # count vectorizer\n",
    "    print('Extracting Features')\n",
    "    \n",
    "    # TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(df['text'].astype(str))\n",
    "    feature_vector = vectorizer.transform(df['text'].astype(str))\n",
    "    \n",
    "    # save vectorizer\n",
    "    pickle.dump(vectorizer, open(\"transformers/twitter/vectorizer.pickle\", \"wb\"))\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ac23e",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(feature_vector,df):\n",
    "    print('Selecting Features')\n",
    "    # defining variables\n",
    "    X = feature_vector\n",
    "    y = df['sentiment']\n",
    "    \n",
    "    # chi2\n",
    "    selector = SelectKBest(chi2,k = 10000)\n",
    "    selector.fit(X,y)\n",
    "    X = selector.transform(X)\n",
    "    \n",
    "    # save selector\n",
    "    pickle.dump(selector, open(\"transformers/twitter/selector.pickle\", \"wb\"))\n",
    "    \n",
    "    # splitting data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61cd456",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdebccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(X_train, y_train):\n",
    "    classifiers = {\n",
    "        'LR' : make_pipeline(LogisticRegression()),\n",
    "        'MNB' : make_pipeline(MultinomialNB()),\n",
    "        'SVM' : make_pipeline(LinearSVC(dual=False))\n",
    "        'MLP' : make_pipeline(MLPClassifier(random_state=1,hidden_layer_sizes=(5000),verbose=True,early_stopping=True)),\n",
    "        'ADA-B' : make_pipeline(AdaBoostClassifier())\n",
    "    }\n",
    "    hyperparameter_grid = {\n",
    "        'LR' : {\n",
    "            'logisticregression__max_iter':[500,1000,1500,2000]\n",
    "        },\n",
    "        'MNB' : {\n",
    "            'multinomialnb__alpha':[0.001,0.005,0.01,0.05,0.10,0.50,0.99,1.0]\n",
    "        },\n",
    "        'SVM' : {\n",
    "            'linearsvc__C': [0.1,1, 10, 100],\n",
    "            'linearsvc__max_iter':[100,250,500,1000,2000,3000] \n",
    "        },\n",
    "        'MLP' :         {\n",
    "            'mlpclassifier__hidden_layer_sizes':[(100),(200),(300),(400),(500)],\n",
    "        },\n",
    "        'ADA-B' : {\n",
    "            'adaboostclassifier__learning_rate':[0.1,0.5,1.0,1.5],\n",
    "        }\n",
    "    }\n",
    "    models = {}\n",
    "    for classifier,pipeline in classifiers.items():\n",
    "        print('starting training {}'.format(classifier))\n",
    "        model = GridSearchCV(pipeline,hyperparameter_grid[classifier])\n",
    "        model.fit(X_train,y_train)\n",
    "        models[classifier] = model\n",
    "        print('finished training {}'.format(classifier))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5e261",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(models,X_test,y_test,df):\n",
    "    for model in models.keys():\n",
    "        predictions = models[model].predict(X_test)\n",
    "        print(predictions)\n",
    "        print('\\n')\n",
    "        \n",
    "        # Accuracy Score\n",
    "        print('{} accuracy score : {} {}'.format(model,accuracy_score(y_test,predictions)*100,'%'))\n",
    "        \n",
    "        # Precision\n",
    "        print('{} Precision score : {} {}'.format(model,precision_score(y_test,predictions, pos_label='positive')*100,'%'))\n",
    "        \n",
    "        # Recall\n",
    "        print('{} Recall score : {} {}'.format(model,recall_score(y_test,predictions,pos_label='positive')*100,'%'))\n",
    "        \n",
    "        # F-1 Score\n",
    "        print('{} F-1 score : {} {}'.format(model,f1_score(y_test,predictions,pos_label='positive')*100,'%'))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test,predictions,labels = df['sentiment'].unique())\n",
    "        df_cm = pd.DataFrame(cm,columns=df['sentiment'].unique(),index = df['sentiment'].unique())\n",
    "        for i in df_cm:\n",
    "            df_cm[i] = df_cm[i]/df_cm[i].sum()\n",
    "        print(model,'Confusion Matrix:','\\n',df_cm)\n",
    "        \n",
    "        print('{} best Hyperarameter: {}'.format(model,models[model].best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e59355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(models):\n",
    "    for model in models.keys():\n",
    "        print('{}'.format(model))\n",
    "        model_name = 'models/final models/binary class/{}.pk1'.format(model)\n",
    "        joblib.dump(models[model],model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_time():\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d323739",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a3295",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load Data - into a data frame\n",
    "data_sets = {\n",
    "    'tweets' : 'TSA_datasets/sentiment140.csv',\n",
    "    'movie_reviews' : 'TSA_datasets/IMDB Dataset.csv',\n",
    "    'electronics' : 'TSA_datasets/Electronics.csv',\n",
    "    'amazon_instant_videos' : 'TSA_datasets/Amazon_Instant_Video.csv',\n",
    "    'beauty' : 'TSA_datasets/beauty.csv',\n",
    "    'cell_phones_and_accessories' : 'TSA_datasets/cell_phones_and_accessories.csv',\n",
    "    'clothing_shoes_and_jewelry' : 'TSA_datasets/clothing_shoes_and_jewelry.csv',\n",
    "    'health_and_personal_care' : 'TSA_datasets/health_and_personal_care.csv',\n",
    "    'sports_and_outdoors' : 'TSA_datasets/sports_and_outdoors.csv'  \n",
    "}\n",
    "\n",
    "# Clean Data - pre-processing data before training\n",
    "df = cleaning_data(df)\n",
    "\n",
    "# Feature Extraction - returns a feature vector\n",
    "feature_vector = feature_extraction(df)\n",
    "\n",
    "# Loadind data from cleaned file\n",
    "# df = pd.read_csv('TSA_datasets/clean_twitter_data.csv')\n",
    "\n",
    "# Feature Selection - returns a feature vector with a subset of the most important features(data splitting as well)\n",
    "X_train, X_test, y_train, y_test = feature_selection(feature_vector,df)\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "# Classification - multiple different classification algorithms applied on to train the model(dict for classifiers and HPG)\n",
    "models = classification(X_train, y_train)\n",
    "\n",
    "# Evaluation - evaluation of the different trained models\n",
    "evaluation(models,X_test,y_test,df)\n",
    "\n",
    "# Save Models\n",
    "save_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac973d90",
   "metadata": {},
   "source": [
    "# convert to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38074b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv():\n",
    "    files = {\n",
    "        'beauty' : 'TSA_datasets/json/Beauty.json',\n",
    "        'cell_phones_and_accessories' : 'TSA_datasets/json/Cell_Phones_and_Accessories.json',\n",
    "        'clothing_shoes_and_jewelry' : 'TSA_datasets/json/Clothing_Shoes_and_Jewelry.json',\n",
    "        'health_and_personal_care' : 'TSA_datasets/json/Health_and_Personal_Care.json',\n",
    "        'sports_and_outdoors' : 'TSA_datasets/json/Sports_and_Outdoors.json',\n",
    "        'toys_and_games' : 'TSA_datasets/json/Toys_and_Games.json'\n",
    "    }\n",
    "    for file in files.keys():\n",
    "        print('{}'.format(file))\n",
    "        df = pd.read_json(files[file],lines=True)\n",
    "        df.to_csv('TSA_datasets/{}.csv'.format(file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

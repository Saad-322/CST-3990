{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda8535b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0073919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import string,re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,f1_score,precision_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c161390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(file_path):\n",
    "    print('Loading data')\n",
    "    df = pd.read_csv(file_path,encoding='latin-1',names=['rating','user_id','date','query','user','text'])\n",
    "    df = pd.DataFrame(columns=['text','rating'],data=df)\n",
    "    df['rating']=df['rating'].apply(lambda x: set_polarity(x))\n",
    "    return df\n",
    "def set_polarity(rating):\n",
    "    if rating == 4:\n",
    "        return 1\n",
    "    elif rating == 0:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf88232",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd54138",
   "metadata": {},
   "source": [
    "### lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed08b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(df):\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4aacc7",
   "metadata": {},
   "source": [
    "### remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33385eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(df):\n",
    "    print('removing punctuation')\n",
    "    df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24652bcd",
   "metadata": {},
   "source": [
    "### remove hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc680cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlinks_usernames(df):\n",
    "    print('removing hyperlinks and usernames')\n",
    "    df['text'] = df['text'].apply(lambda x: remove_h_u(x))\n",
    "    return df\n",
    "def remove_h_u(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links = re.findall(link_regex,text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0],'')\n",
    "    prefix = ['@']\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in prefix:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c200bc",
   "metadata": {},
   "source": [
    "### remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fbe59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(df):\n",
    "    print('removing emojis')\n",
    "    df['text'] = df['text'].apply(lambda x: remove_e(x))\n",
    "    return df\n",
    "def remove_e(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f76e5",
   "metadata": {},
   "source": [
    "### remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10e6994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df):\n",
    "    print('removing stop words')\n",
    "    df['text'] = df['text'].apply(lambda x: remove_sw(x))\n",
    "    return df\n",
    "def remove_sw(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        if word not in stop_words:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a591216",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9831952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(df):\n",
    "    print('tokenizing')\n",
    "    df['text'] = df['text'].apply(lambda x: perform_t(x))\n",
    "    return df\n",
    "def perform_t(text):\n",
    "    text = re.split('\\W',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227e7e1",
   "metadata": {},
   "source": [
    "### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade46c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(df):\n",
    "    print('lemmatizing text')\n",
    "    df['text'] = df['text'].apply(lambda x: perform_l(x))\n",
    "    return df\n",
    "def perform_l(text):\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    text = [wnl.lemmatize(token) for token in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532fbe0",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d33bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(df):\n",
    "    print('stemming text')\n",
    "    df['text'] = df['text'].apply(lambda x: perform_s(x))\n",
    "    return df\n",
    "def perform_s(text):\n",
    "    sp = nltk.PorterStemmer()\n",
    "    text =[sp.stem(token) for token in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc549d33",
   "metadata": {},
   "source": [
    "### data cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dceea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(df):\n",
    "    lower_case(df)\n",
    "    remove_hyperlinks_usernames(df)\n",
    "    remove_punctuations(df)\n",
    "    remove_emojis(df)\n",
    "    remove_stopwords(df)\n",
    "    tokenization(df)\n",
    "    lemmatization(df)\n",
    "    stemming(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87dfefd",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc3eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df):\n",
    "    # count vectorizer\n",
    "    print('Extracting Features')\n",
    "#     cv = CountVectorizer()\n",
    "#     cv.fit(df['text'].astype(str))\n",
    "#     feature_vector = cv.transform(df['text'].astype(str))\n",
    "    \n",
    "    # TF-IDF\n",
    "    tf_idf = TfidfVectorizer()\n",
    "    tf_idf.fit(df['text'].astype(str))\n",
    "    feature_vector = tf_idf.transform(df['text'].astype(str))\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f97a3",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e40e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(feature_vector,df):\n",
    "    print('Selecting Features')\n",
    "    # defining variables\n",
    "    X = feature_vector\n",
    "    y = df['rating']\n",
    "    \n",
    "    # chi2\n",
    "    X = SelectKBest(chi2,k=100).fit_transform(X,y)\n",
    "    \n",
    "    # splitting data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "    \n",
    "    # standardizing data\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd86f8",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08f85159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(X_train, X_test, y_train, y_test):\n",
    "    classifiers = {\n",
    "#         'LR' : make_pipeline(LogisticRegression()),\n",
    "#         'MNB' : make_pipeline(MultinomialNB()),\n",
    "#         'SVM' : make_pipeline(LinearSVC(dual=False)),\n",
    "        'DT' : make_pipeline(DecisionTreeClassifier())\n",
    "#         'RSS' : make_pipeline(BaggingClassifier())\n",
    "    }\n",
    "    hyperparameter_grid = {\n",
    "        'LR' : {\n",
    "            'logisticregression__max_iter':[500,1000,1500,2000]\n",
    "        },\n",
    "        'MNB' : {\n",
    "            'multinomialnb__alpha':[0.001,0.005,0.01,0.05,0.10,0.50,0.99,1.0]\n",
    "        },\n",
    "        'SVM' : {\n",
    "            'linearsvc__C': [0.1,1, 10, 100],\n",
    "            'linearsvc__max_iter':[100,250,500,1000,2000,3000] \n",
    "        },\n",
    "        'DT' : {\n",
    "#             'decisiontreeclassifier__criterion' : ['gini','entropy'],\n",
    "#             'decisiontreeclassifier__splitter' : ['best','random']\n",
    "        },\n",
    "        'RSS' : {\n",
    "            'baggingclassifier__max_samples' : [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "            'baggingclassifier__n_estimators' : [10, 50, 100, 500, 500, 1000, 5000]\n",
    "#             'baggingclassifier__base_estimator' : [KNeighborsClassifier(n_neighbors=1),KNeighborsClassifier(n_neighbors=5),KNeighborsClassifier(n_neighbors=15),KNeighborsClassifier(n_neighbors=20)]\n",
    "        }\n",
    "    }\n",
    "    models = {}\n",
    "    for classifier,pipeline in classifiers.items():\n",
    "        print('starting training {}'.format(classifier))\n",
    "        model = GridSearchCV(pipeline,hyperparameter_grid[classifier])\n",
    "        model.fit(X_train,y_train)\n",
    "        models[classifier] = model\n",
    "        print('finished training {}'.format(classifier))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc396af",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "019d8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(models,X_test,y_test,df):\n",
    "    # Accuracy Score\n",
    "    for model in models.keys():\n",
    "        predictions = models[model].predict(X_test)\n",
    "        print('\\n')\n",
    "        print('{} accuracy score : {} {}'.format(model,accuracy_score(y_test,predictions)*100,'%'))\n",
    "        \n",
    "        # Precision\n",
    "        print('{} Precision score : {} {}'.format(model,precision_score(y_test,predictions)*100,'%'))\n",
    "        \n",
    "        # Recall\n",
    "        print('{} Recall score : {} {}'.format(model,recall_score(y_test,predictions)*100,'%'))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test,predictions,labels = df['rating'].unique())\n",
    "        df_cm = pd.DataFrame(cm,columns=df['rating'].unique(),index = df['rating'].unique())\n",
    "        for i in df_cm:\n",
    "            df_cm[i] = df_cm[i]/df_cm[i].sum()\n",
    "        print(model,'\\n',df_cm)\n",
    "\n",
    "        # F-measure\n",
    "        \n",
    "        # ROC Area \n",
    "        # TP rate, FP rate, \n",
    "        \n",
    "        print('{} best Hyperarameter: {}'.format(model,models[model].best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49deae7",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a048854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features\n",
      "Selecting Features\n",
      "(1280000, 100) (320000, 100) (1280000,) (320000,)\n",
      "starting training DT\n",
      "finished training DT\n",
      "\n",
      "\n",
      "DT accuracy score : 65.07843749999999 %\n",
      "DT Precision score : 61.924049632498225 %\n",
      "DT Recall score : 78.3383011829106 %\n",
      "DT \n",
      "           -1        1\n",
      "-1  0.705109  0.38076\n",
      " 1  0.294891  0.61924\n",
      "DT best Hyperarameter: Pipeline(steps=[('decisiontreeclassifier', DecisionTreeClassifier())])\n"
     ]
    }
   ],
   "source": [
    "# Load Data - into a data frame\n",
    "# file_name = 'twitter/sentiment140.csv'\n",
    "# df = loading_data(file_name)\n",
    "\n",
    "# # Clean Data - pre-processing data before training\n",
    "# df = cleaning_data(df)\n",
    "\n",
    "df = pd.read_csv('clean_text1.csv')\n",
    "# df = df[df['rating']==1].head(25000).append(df[df['rating']==-1].head(25000))\n",
    "\n",
    "# Feature Extraction - returns a feature vector\n",
    "feature_vector = feature_extraction(df)\n",
    "\n",
    "# Feature Selection - returns a feature vector with a subset of the most important features(data splitting as well)\n",
    "X_train, X_test, y_train, y_test = feature_selection(feature_vector,df)\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "# Classification - multiple different classification algorithms applied on to train the model(dict for classifiers and HPG)\n",
    "models = classification(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Evaluation - evaluation of the different trained models\n",
    "evaluation(models,X_test,y_test,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7917de6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 18:41:36\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
